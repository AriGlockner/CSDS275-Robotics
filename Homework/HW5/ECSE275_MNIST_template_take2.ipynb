{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ECSE 275 Assignment 5 Part 3\n",
        "\n",
        "In this google colab notebook, we train a neural network to recognize numbers. We will later use this trained neural network to imbue a robot with the ability to recognize numbers."
      ],
      "metadata": {
        "id": "eSV4nxC6orgH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvN_ShIMc1ro"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def visualize_mnist_grid(data_loader, num_samples=25, num_rows=5):\n",
        "    # Create an iterator from the data loader\n",
        "    data_iterator = iter(data_loader)\n",
        "\n",
        "    # Get a batch of images and labels\n",
        "    images, labels = next(data_iterator)\n",
        "\n",
        "    # Display the images in a grid\n",
        "    fig, axes = plt.subplots(num_rows, num_samples // num_rows, figsize=(12, 8))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        row, col = i // (num_samples // num_rows), i % (num_samples // num_rows)\n",
        "        ax = axes[row, col]\n",
        "        image = images[i].numpy().squeeze()\n",
        "        label = labels[i].item()\n",
        "\n",
        "        ax.imshow(image, cmap='gray')\n",
        "        ax.set_title(f'Label: {label}')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to use GPU\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tdeJDhyi93G",
        "outputId": "283c7f0e-02ad-44b0-d9b8-e2708d5e7641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the MNIST data set of numbers. We load the dataset for use and visualize it to give us an idea of what we are dealing with."
      ],
      "metadata": {
        "id": "8CZBIF9adB_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#transform = transforms.Compose([transforms.ToTensor(),transforms.RandomResizedCrop(size=(24, 24))])\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Visualize a grid of 25 samples from the MNIST dataset\n",
        "visualize_mnist_grid(trainloader, num_samples=25, num_rows=5)"
      ],
      "metadata": {
        "id": "_klwBI12dBaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2: Fully Connected Neural Network\n",
        "\n",
        "Define a neural network architecture with 3 Linear Layers\n",
        "\n",
        "Use the following intermediate layer architecture:\n",
        "\n",
        "1.   First Layer 128 nodes\n",
        "2.   Second Layer 64 nodes\n",
        "\n",
        "**What dimension is input to the first layer if we are going to flatten the image into a single vector?**\n",
        "\n",
        "**What should the output be if we are trying to predict 10 different numbers with the softmax function?**\n",
        "\n",
        "Do build the network you have to construct a custom `Net` class which inherits from the `nn.Module` class\n",
        "\n",
        "During the `init` method use the function to declare layers: `nn.Linear(input_dim,output_dim)`\n",
        "\n",
        "In the `forward` method, pass the input through each layer by calling the layers in sequence.\n",
        "e.g `x = self.fc(x) `\n",
        "\n",
        "After each pass through an intermediate layer, remember to then pass the network through a non-linearity. Here we use the Rectified Linear Unit\n",
        "e.g. `x = F.relu(x)`\n",
        "\n",
        "The output of the last layer does not need to be pass through a non-linearity\n",
        "Just return the value of x after we pass all the layers.\n"
      ],
      "metadata": {
        "id": "IBrnGhQwdQLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(????)\n",
        "        self.fc2 = nn.Linear(????)\n",
        "        self.fc3 = nn.Linear(????)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)  # Flatten the input\n",
        "        x = # Linear Layer ReLU\n",
        "        x = # Linear Layer ReLU\n",
        "        x = # Linear Layer\n",
        "        return x\n",
        "\n",
        "net = Net().to(device)\n",
        "net\n"
      ],
      "metadata": {
        "id": "0fU14B_hdRbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the loss criteria and our optimization method\n",
        "\n",
        "The loss we use is the Cross Entropy Loss. This automatically uses the Softmax function followed by calculation of the negative log-likelihood loss.\n",
        "\n",
        "For the optimizer, we use the Ada-M AKA Adam optimizer because it does not require us to really tune the learning rate for our training."
      ],
      "metadata": {
        "id": "udykb2OsdVe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "Wm8hEngddU4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the neural network\n",
        "\n",
        "We will run the neural network training for 10 passes (epochs) through our data.\n",
        "\n",
        "We monitor the loss over each epoch"
      ],
      "metadata": {
        "id": "ixs8DzwcmmBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):  # You can adjust the number of epochs\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(trainloader)}')"
      ],
      "metadata": {
        "id": "I6r-etkOdWoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate our performance on the test dataset\n",
        "\n",
        "Next, we create a new dataloader for the MNIST Test Set and run an evaluation of our trained neural network.\n",
        "\n",
        "**Report your accuracy on the test set.**"
      ],
      "metadata": {
        "id": "N8EhoRsAmq67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on the test set: {100 * correct / total}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY2esZyAem46",
        "outputId": "cfcfa884-ce2d-4b08-a8fe-8a9ffeff27e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 97.81%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save our model parameters\n",
        "\n",
        "We can save our model parameters to a specific file name.\n",
        "\n",
        "Make sure to save these weights to your local drive."
      ],
      "metadata": {
        "id": "Oeh3iFjamtf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"model_weights.pth\"\n",
        "\n",
        "net = net.to(\"cpu\")\n",
        "torch.save(net.state_dict(),filename)"
      ],
      "metadata": {
        "id": "PKx3AhNPmL00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3: Train a Convolutional Neural Network\n",
        "\n",
        "Next we will train a more powerful model for image recognition, the convolutional neural network.\n",
        "\n",
        "We will use the following architecture\n",
        "\n",
        "1.   Convolution Layer 32-channel with stride 5\n",
        "2.   Maxpool kernel size 2, stride 2\n",
        "3.   Convolution Layer 64-channel with stride 3\n",
        "4.   Maxpool kernel size 2, stride 2\n",
        "5.   Convolution Layer 64-channel with stride 1\n",
        "6.   Maxpool kernel size 2, stride 2\n",
        "7.   Linear Layer 128 nodes\n",
        "\n",
        "After each neural network layer (i.e. convolutional linear layers) we must pass the output through a Rectified Linear Unit like we did in with the fully connected neural network in Question 2.\n",
        "\n",
        "Convolutional layers can be defined using `nn.Conv2d(input_channel,output_channel,stride)`\n",
        "\n",
        "Maxpools can be performed using `nn.MaxPool2d(kernel size,stride)`\n",
        "\n",
        "**What is the final size of the output of the final pool layer? Note: This is the input size to the first linear layer**"
      ],
      "metadata": {
        "id": "mi9-YU-kPgXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(???)\n",
        "        self.pool1 = nn.MaxPool2d(???)\n",
        "        self.conv2 = nn.Conv2d(???)\n",
        "        self.pool2 = nn.MaxPool2d(???)\n",
        "        self.conv3 = nn.Conv2d(???)\n",
        "        self.pool3 = nn.MaxPool2d(???)\n",
        "        self.fc1 = nn.Linear(???)\n",
        "        self.fc2 = nn.Linear(???)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = # Conv ReLU Pool\n",
        "        x = # Conv ReLU Pool\n",
        "        x = # Conv ReLU Pool\n",
        "        x = x.view(-1, ???)\n",
        "        x = # ReLU Linear Layer\n",
        "        x = # Linear Layer\n",
        "        return x\n",
        "\n",
        "\n",
        "try:\n",
        "  del net\n",
        "  del loss\n",
        "except:\n",
        "  pass\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "net = CNN().to(device)\n",
        "\n",
        "net"
      ],
      "metadata": {
        "id": "eilAx-nAiJBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define the loss criteria and our optimization method\n",
        "The loss we use is the Cross Entropy Loss. This automatically uses the Softmax function followed by calculation of the negative log-likelihood loss.\n",
        "\n",
        "For the optimizer, we use the Ada-M AKA Adam optimizer because it does not require us to really tune the learning rate for our training."
      ],
      "metadata": {
        "id": "vTwsOhnxReWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "0ctJVpFHSK7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the neural network\n",
        "\n",
        "We will run the neural network training for 10 passes (epochs) through our data.\n",
        "\n",
        "We monitor the loss over each epoch"
      ],
      "metadata": {
        "id": "Bi35pU1GSXEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):  # You can adjust the number of epochs\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(trainloader)}')"
      ],
      "metadata": {
        "id": "xbjoHRXWSXzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate our performance on the test dataset\n",
        "\n",
        "Next, we create a new dataloader for the MNIST Test Set and run an evaluation of our trained neural network.\n",
        "\n",
        "**Report your accuracy on the test set**"
      ],
      "metadata": {
        "id": "o-aWhBN2SdO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on the test set: {100 * correct / total}%')"
      ],
      "metadata": {
        "id": "hCm0SGLwSlWR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7778c8ef-e18c-4883-9ed2-b6c8a8e5a989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the test set: 99.21%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save our model parameters\n",
        "\n",
        "We can save our model parameters to a specific file name.\n",
        "\n",
        "Make sure to save these weights to your local drive."
      ],
      "metadata": {
        "id": "ItgIyGfYSkiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"model_weights_cnn.pth\"\n",
        "\n",
        "net = net.to(\"cpu\")\n",
        "torch.save(net.state_dict(),filename)"
      ],
      "metadata": {
        "id": "GdDacOH7Scuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4: Train with data augmentation\n",
        "\n",
        "Finally we train a Convolutional Neural Network, but this time we create variation in our training data using data augmentation.\n",
        "\n",
        "We will use a crop augmentation which will randomly crop the training images and rescale them so that the neural network is resistant to scale changes in the numbers.\n",
        "\n",
        "Below we declare and then visualize the augmented dataset."
      ],
      "metadata": {
        "id": "tlI8a_RQSvNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),transforms.RandomResizedCrop(size=(28, 28),scale=(0.4,1.0))])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Visualize a grid of 25 samples from the MNIST dataset\n",
        "visualize_mnist_grid(trainloader, num_samples=25, num_rows=5)"
      ],
      "metadata": {
        "id": "O5tfxrxLTYim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and Evaluation\n",
        "\n",
        "We go through the training process using the same convolutional network architecture we used in Question 3.\n",
        "\n",
        "**Report the accuracy on the test set.**\n",
        "\n",
        "**Compared to the previous networks, how is the accuracy different? Why do you think this is so?**"
      ],
      "metadata": {
        "id": "m-hN5irbTeZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-declare the CNN object, the loss, and the optimizer\n",
        "try:\n",
        "  del net\n",
        "  del loss\n",
        "except:\n",
        "  pass\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "net = CNN()\n",
        "net = net.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# Run the training\n",
        "for epoch in range(10):  # You can adjust the number of epochs\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(trainloader)}')\n",
        "\n",
        "# Evaluate on the test set\n",
        "transform = transforms.Compose([transforms.ToTensor()]) # remove the data augmentation transformation\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy on the test set: {100 * correct / total}%')"
      ],
      "metadata": {
        "id": "OmYgx2IWTeHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save the model\n",
        "\n",
        "We can save our model parameters to a specific file name.\n",
        "\n",
        "Make sure to save these weights to your local drive."
      ],
      "metadata": {
        "id": "bgy0KGh4UNSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"model_weights_cnn_aug.pth\"\n",
        "\n",
        "net = net.to(\"cpu\")\n",
        "torch.save(net.state_dict(),filename)"
      ],
      "metadata": {
        "id": "PygfPm4BUNEt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}